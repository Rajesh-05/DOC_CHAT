# Doc_chat

## Introduction
**Doc_chat** is an advanced question-answering system that leverages the power of Retrieval-Augmented Generation (RAG) and Groq's llama3 as a Large Language Model (LLM). It is uniquely capable of handling **multiple PDFs simultaneously**, providing answers with references to a diverse range of documents.

## Features
- **Multi-PDF Handling**: **Doc_chat** can process and reference **multiple PDFs at once**, allowing for comprehensive answers that draw from an extensive knowledge base.
- **Retrieval-Augmented Generation**: Combines the benefits of pre-trained language models with external knowledge retrieval to generate informative responses.
- **Groq's llama3 Integration**: Utilizes the computational efficiency and scalability of Groq's llama3 as the underlying LLM.
- **Pinecone Vector Database**: Incorporates **Pinecone** as a vector database to efficiently manage and retrieve data for the question-answering process.
- **Scalable Architecture**: Designed to handle increasing loads and complex queries with ease.
- **User-Friendly Interface**: Simple and intuitive interaction flow for end-users.

## Sample Working Video
For a demonstration of **Doc_chat** in action, check out this sample working video.

https://drive.google.com/file/d/1JliIfZPSzx0518QIQ80jiIBHZH3ZRYrm/view?usp=sharing

## Installation
To set up Doc_chat on your local machine, follow these steps:
1. Clone the repository:

```git clone https://github.com/yourusername/Doc_chat.git```

2. Navigate to the project directory:

```cd DOC_CHAT```

3. Install the required dependencies:

```pip install -r requirements.txt```

4. Run app.ipynb